{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa1330c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a88a46d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indiscernibility(matrix,y):\n",
    "    df=pd.DataFrame(matrix)\n",
    "    # this will return the list of columns     \n",
    "    y=list(df.columns)\n",
    "    grouped_df=df.groupby(y)\n",
    "    \n",
    "    ind_R=list(list())\n",
    "    for key, item in grouped_df:\n",
    "#         print(grouped_df.get_group(key), \"\\n\",grouped_df.get_group(key).index ,\"\\n\\n\")\n",
    "        lis=[]\n",
    "        for i in grouped_df.get_group(key).index:\n",
    "            lis.append(i)\n",
    "        ind_R.append(list(lis))\n",
    "    return ind_R\n",
    "\n",
    "def encoding_discourse_type(x):\n",
    "    if x==\"Lead\":\n",
    "        return 0\n",
    "    if x==\"Position\":\n",
    "        return 1\n",
    "    if x==\"Evidence\":\n",
    "        return 2\n",
    "    if x==\"Claim\":\n",
    "        return 3\n",
    "    if x==\"Concluding Statement\":\n",
    "        return 4\n",
    "    if x==\"Counterclaim\":\n",
    "        return 5\n",
    "    if x=='Rebuttal':\n",
    "        return 6\n",
    "    \n",
    "def stemming_stopwords_removing(df):\n",
    "    corpus=[]\n",
    "    for i in range(len(df)):\n",
    "        review=re.sub('[^a-zA-Z]',' ',df[\"discourse_text\"][i])\n",
    "        review=review.lower()\n",
    "        review=review.split()\n",
    "        ps=PorterStemmer()\n",
    "        all_stopwords=stopwords.words(\"english\")\n",
    "        review=[ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
    "        review=' '.join(review)\n",
    "        corpus.append(review)\n",
    "    return corpus\n",
    "\n",
    "# storing the total occurrence.......\n",
    "def get_total_index_words(corpus):\n",
    "    index_word={}\n",
    "    for i in corpus:\n",
    "        s=i.split()\n",
    "        for j in s:\n",
    "            if j not in index_word:\n",
    "                index_word[j]=1\n",
    "            else:\n",
    "                index_word[j]+=1\n",
    "    return index_word\n",
    "    \n",
    "def get_values(dataset,threshold=1):\n",
    "    \n",
    "    # taking sample of 20 documents for lead category....\n",
    "    df = dataset\n",
    "\n",
    "    total_corpus = stemming_stopwords_removing(df)\n",
    "    # print(total_corpus)\n",
    "\n",
    "    # getting total index words and their count in the taken sample as a dict\n",
    "    total_index_words = get_total_index_words(total_corpus)\n",
    "    # print(len(lead_index_words))\n",
    "\n",
    "    # Creating a list of total keywords before filtering..\n",
    "    total_keywords = list(total_index_words.keys())\n",
    "\n",
    "    # Creating a matrix of width equals len(lead_keywords)\n",
    "    matrix=np.zeros((len(df),len(total_keywords)))\n",
    "\n",
    "    \n",
    "    # Storing occurrence of each term in each document respectively\n",
    "    for i in range(len(total_corpus)):\n",
    "        s = total_corpus[i].split()\n",
    "        for h in s:\n",
    "            j = total_keywords.index(h)\n",
    "            matrix[i,j] += 1\n",
    "\n",
    "\n",
    "    # Storing their weights....\n",
    "    weighted_matrix = np.copy(matrix)\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        for j in range(len(total_keywords)):\n",
    "            weighted_matrix[i,j] = weighted_matrix[i,j] / total_index_words[total_keywords[j]]\n",
    "    #            print(weighted_matrix[i,j])\n",
    "\n",
    "\n",
    "\n",
    "    # FILTERING WEIGHTS with a threshold.......\n",
    "    valid_index = []\n",
    "    for i in range(len(df)):\n",
    "        for j in range(len(total_keywords)):\n",
    "            if weighted_matrix[i,j] >= threshold:\n",
    "                valid_index.append(j)\n",
    "\n",
    "    # removing duplicates and storing them in a list.......    \n",
    "    valid_index = list(set(valid_index))\n",
    "\n",
    "\n",
    "    # # Storing the final keywords.... \n",
    "    valid_index_words = []\n",
    "    for i in range(len(valid_index)):\n",
    "        valid_index_words.append(total_keywords[valid_index[i]])\n",
    "    # print(valid_lead_index_words)\n",
    "\n",
    "    return total_keywords,total_index_words,matrix, weighted_matrix, valid_index, valid_index_words\n",
    "\n",
    "def get_test_matrix(dataset,total_sample_keywords,threshold=1):\n",
    "      # taking sample of 20 documents for lead category....\n",
    "    df = dataset\n",
    "\n",
    "    total_corpus = stemming_stopwords_removing(df)\n",
    "    # print(total_corpus)\n",
    "\n",
    "    # getting total index words and their count in the taken sample as a dict\n",
    "    total_index_words = get_total_index_words(total_corpus)\n",
    "\n",
    "\n",
    "    # Creating a list of total keywords before filtering..\n",
    "    total_keywords = total_sample_keywords\n",
    "    # print(lead_keywords)\n",
    "\n",
    "    # Creating a matrix of width equals len(lead_keywords)\n",
    "    matrix=np.zeros((len(df),len(total_keywords)))\n",
    "    print(len(df),len(total_keywords))\n",
    "\n",
    "    \n",
    "    # Storing occurrence of each term in each document respectively\n",
    "    for i in range(len(total_corpus)):\n",
    "        s = total_corpus[i].split()\n",
    "        for h in s:\n",
    "            try:\n",
    "                j = total_keywords.index(h)\n",
    "                matrix[i,j] += 1\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "    # Storing their weights....\n",
    "    weighted_matrix = np.copy(matrix)\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        for j in range(len(total_keywords)):\n",
    "            try:\n",
    "                weighted_matrix[i,j] = weighted_matrix[i,j] / total_index_words[total_keywords[j]]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "\n",
    "    # FILTERING WEIGHTS with a threshold.......\n",
    "    valid_index = []\n",
    "    for i in range(len(df)):\n",
    "        for j in range(len(total_keywords)):\n",
    "            if weighted_matrix[i,j] >= threshold:\n",
    "                valid_index.append(j)\n",
    "\n",
    "    # removing duplicates and storing them in a list.......    \n",
    "    valid_index = list(set(valid_index))\n",
    "    \n",
    "\n",
    "\n",
    "    # # Storing the final keywords.... \n",
    "    valid_index_words = []\n",
    "    for i in range(len(valid_index)):\n",
    "        valid_index_words.append(total_keywords[valid_index[i]])\n",
    "    # print(valid_lead_index_words)\n",
    "\n",
    "    return total_keywords,total_index_words,matrix, weighted_matrix, valid_index, valid_index_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebb6b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_element_indiscernibility(matrix,index):\n",
    "    df=pd.DataFrame(matrix)\n",
    "    # this will return the list of columns     \n",
    "    y=list(df.columns)\n",
    "    \n",
    "    grouped_df=df.groupby(y[index])\n",
    "    \n",
    "    ind_R=list(list())\n",
    "    for key, item in grouped_df:\n",
    "    #print(grouped_df.get_group(key), \"\\n\",grouped_df.get_group(key).index ,\"\\n\\n\")\n",
    "        lis=[]\n",
    "        for i in grouped_df.get_group(key).index:\n",
    "            lis.append(i)\n",
    "        ind_R.append(list(lis))\n",
    "    return ind_R\n",
    "\n",
    "def indiscernibility(matrix,y):\n",
    "    if len(y)==0:\n",
    "        return []\n",
    "    df=pd.DataFrame(matrix)\n",
    "    # this will return the list of columns     \n",
    "    grouped_df=df.groupby(y)\n",
    "    ind_R=list(list())\n",
    "    for key, item in grouped_df:\n",
    "    #print(grouped_df.get_group(key), \"\\n\",grouped_df.get_group(key).index ,\"\\n\\n\")\n",
    "        lis=[]\n",
    "        for i in grouped_df.get_group(key).index:\n",
    "            lis.append(i)\n",
    "        ind_R.append(list(lis))\n",
    "    return ind_R\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43474fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>230.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>313.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>Some certain areas in the United States ban ph...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>402.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>When people have phones, they know about certa...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 2</td>\n",
       "      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>759.0</td>\n",
       "      <td>886.0</td>\n",
       "      <td>Driving is one of the way how to get around. P...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>144288</td>\n",
       "      <td>4C471936CD75</td>\n",
       "      <td>1.618153e+12</td>\n",
       "      <td>2234.0</td>\n",
       "      <td>3203.0</td>\n",
       "      <td>if I'm not sure what college I want to attend...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 2</td>\n",
       "      <td>386 387 388 389 390 391 392 393 394 395 396 39...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>144289</td>\n",
       "      <td>4C471936CD75</td>\n",
       "      <td>1.618153e+12</td>\n",
       "      <td>3221.0</td>\n",
       "      <td>4509.0</td>\n",
       "      <td>seeking multiple opinions before making a har...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 3</td>\n",
       "      <td>576 577 578 579 580 581 582 583 584 585 586 58...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>144290</td>\n",
       "      <td>4C471936CD75</td>\n",
       "      <td>1.618025e+12</td>\n",
       "      <td>4510.0</td>\n",
       "      <td>4570.0</td>\n",
       "      <td>it is better to seek multiple opinions instead...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>828 829 830 831 832 833 834 835 836 837 838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>144291</td>\n",
       "      <td>4C471936CD75</td>\n",
       "      <td>1.618025e+12</td>\n",
       "      <td>4570.0</td>\n",
       "      <td>4922.0</td>\n",
       "      <td>The impact of asking people to help you make a...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 4</td>\n",
       "      <td>839 840 841 842 843 844 845 846 847 848 849 85...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>144292</td>\n",
       "      <td>4C471936CD75</td>\n",
       "      <td>1.618025e+12</td>\n",
       "      <td>4935.0</td>\n",
       "      <td>5825.0</td>\n",
       "      <td>there are many other reasons one might want to...</td>\n",
       "      <td>Concluding Statement</td>\n",
       "      <td>Concluding Statement 1</td>\n",
       "      <td>905 906 907 908 909 910 911 912 913 914 915 91...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index            id  discourse_id  discourse_start  discourse_end  \\\n",
       "0         0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "1         1  423A1CA112E2  1.622628e+12            230.0          312.0   \n",
       "2         2  423A1CA112E2  1.622628e+12            313.0          401.0   \n",
       "3         3  423A1CA112E2  1.622628e+12            402.0          758.0   \n",
       "4         4  423A1CA112E2  1.622628e+12            759.0          886.0   \n",
       "..      ...           ...           ...              ...            ...   \n",
       "995  144288  4C471936CD75  1.618153e+12           2234.0         3203.0   \n",
       "996  144289  4C471936CD75  1.618153e+12           3221.0         4509.0   \n",
       "997  144290  4C471936CD75  1.618025e+12           4510.0         4570.0   \n",
       "998  144291  4C471936CD75  1.618025e+12           4570.0         4922.0   \n",
       "999  144292  4C471936CD75  1.618025e+12           4935.0         5825.0   \n",
       "\n",
       "                                        discourse_text        discourse_type  \\\n",
       "0    Modern humans today are always on their phone....                  Lead   \n",
       "1    They are some really bad consequences when stu...              Position   \n",
       "2    Some certain areas in the United States ban ph...              Evidence   \n",
       "3    When people have phones, they know about certa...              Evidence   \n",
       "4    Driving is one of the way how to get around. P...                 Claim   \n",
       "..                                                 ...                   ...   \n",
       "995   if I'm not sure what college I want to attend...              Evidence   \n",
       "996   seeking multiple opinions before making a har...              Evidence   \n",
       "997  it is better to seek multiple opinions instead...              Position   \n",
       "998  The impact of asking people to help you make a...              Evidence   \n",
       "999  there are many other reasons one might want to...  Concluding Statement   \n",
       "\n",
       "         discourse_type_num                                   predictionstring  \n",
       "0                    Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
       "1                Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n",
       "2                Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n",
       "3                Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n",
       "4                   Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...  \n",
       "..                      ...                                                ...  \n",
       "995              Evidence 2  386 387 388 389 390 391 392 393 394 395 396 39...  \n",
       "996              Evidence 3  576 577 578 579 580 581 582 583 584 585 586 58...  \n",
       "997              Position 1        828 829 830 831 832 833 834 835 836 837 838  \n",
       "998              Evidence 4  839 840 841 842 843 844 845 846 847 848 849 85...  \n",
       "999  Concluding Statement 1  905 906 907 908 909 910 911 912 913 914 915 91...  \n",
       "\n",
       "[1000 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.read_csv(\"train.csv\")\n",
    "train_head=train.head(500)\n",
    "train_tail=train.tail(500)\n",
    "train=pd.concat([train_head,train_tail])\n",
    "train.reset_index(inplace=True)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e3d5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now i will send my training data to get its list of final keywords after eliminating useles words from our list\n",
    "# using weighted matrix .....\n",
    "\n",
    "train_total_keywords,train_total_index_words, train_matrix, train_weighted_matrix, train_valid_index, train_final_keywords=get_values(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90f9f80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_weighted_matrix))\n",
    "c_attr=train_weighted_matrix.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2353fa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "train[\"Y\"]=train[\"discourse_type\"].copy()\n",
    "train[\"Y\"]=train[\"Y\"].apply(lambda x: encoding_discourse_type(x))\n",
    "y=train[\"Y\"].values\n",
    "print(len(y))\n",
    "dec_attr=y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "321a3cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5        0.1        0.03703704 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         1.         1.        ]]\n",
      "shape of old training attribute (1000, 2076)\n",
      "\n",
      "\n",
      "No of unique columns: 1564\n",
      "[[0.5        0.         0.         ... 0.         0.         0.        ]\n",
      " [0.1        0.         0.         ... 0.         0.         0.        ]\n",
      " [0.03703704 0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.5        0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         1.        ]]\n",
      "\n",
      " unique_col_table shape (1564, 1000)\n",
      "\n",
      "new train attributes after preserving only one repetitve element:\n",
      " [[0.5        0.1        0.03703704 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.5        0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         1.        ]]\n",
      "\n",
      "shape of new train attributes: (1000, 1564)\n",
      "number of repeated columns are:  298\n"
     ]
    }
   ],
   "source": [
    "# NOW REMOVING IDENTICAL COLUMNS FROM OUR MATRIX INPLACE.......\n",
    "print((c_attr))\n",
    "print(\"shape of old training attribute\",c_attr.shape)\n",
    "grp=np.column_stack(c_attr)\n",
    "temp=pd.DataFrame(grp)\n",
    "# print(temp.shape)\n",
    "\n",
    "temp_grp=indiscernibility(temp,list(temp.columns))\n",
    "# print(temp_grp)\n",
    "\n",
    "unique_columns=[]\n",
    "for i in range(len(temp_grp)):\n",
    "    unique_columns.append(temp_grp[i][0])\n",
    "    \n",
    "unique_columns.sort()\n",
    "print()\n",
    "# print(\"Unique columns:\",unique_columns)\n",
    "print(\"\\nNo of unique columns:\",len(unique_columns))\n",
    "\n",
    "unique_col_table=list(list())\n",
    "for i in unique_columns:\n",
    "    unique_col_table.append(list(grp[i]))\n",
    "unique_col_table=np.array(unique_col_table)\n",
    "# print(\"\\nunique_col_table:\")\n",
    "print(unique_col_table)\n",
    "print(\"\\n unique_col_table shape\",unique_col_table.shape)\n",
    "\n",
    "new_train_attributes=np.column_stack(unique_col_table)\n",
    "\n",
    "print(\"\\nnew train attributes after preserving only one repetitve element:\\n\",new_train_attributes)\n",
    "print(\"\\nshape of new train attributes:\",new_train_attributes.shape)\n",
    "c_attr=new_train_attributes.copy()\n",
    "\n",
    "repetitive_columns=list(set(list(range(len(c_attr[0]))))-set(unique_columns))\n",
    "print(\"number of repeated columns are: \", len(repetitive_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18001b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1564)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(c_attr.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef6e6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from frlearn.base import probabilities_from_scores, select_class\n",
    "from frlearn.classifiers import FRNN\n",
    "from frlearn.feature_preprocessors import RangeNormaliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2210f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(c_attr, y, stratify=y, random_state=0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "570f9a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 3, 4, 3, 2, 4, 3, 3, 3, 1, 1, 2, 0, 3, 3, 3, 6, 2, 2, 2,\n",
       "       3, 2, 0, 2, 3, 5, 3, 6, 4, 2, 2, 1, 2, 2, 2, 3, 3, 1, 2, 4, 2, 3,\n",
       "       0, 3, 3, 1, 3, 3, 4, 1, 3, 3, 1, 1, 3, 2, 3, 0, 2, 3, 2, 2, 0, 2,\n",
       "       3, 3, 2, 2, 4, 2, 1, 3, 2, 1, 0, 3, 2, 1, 0, 2, 3, 2, 3, 0, 2, 3,\n",
       "       0, 2, 1, 3, 3, 3, 1, 0, 4, 3, 2, 3, 2, 2, 3, 2, 1, 4, 4, 3, 0, 3,\n",
       "       2, 3, 3, 2, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3, 4, 3, 2, 1, 3, 4, 3, 3,\n",
       "       2, 4, 2, 4, 2, 2, 0, 2, 3, 3, 2, 4, 3, 5, 2, 2, 3, 2, 0, 0, 3, 2,\n",
       "       4, 4, 4, 2, 2, 3, 2, 2, 4, 2, 2, 1, 3, 2, 3, 3, 2, 2, 2, 0, 1, 3,\n",
       "       0, 1, 3, 2, 2, 1, 0, 3, 3, 3, 1, 4, 2, 3, 1, 3, 2, 2, 2, 4, 3, 3,\n",
       "       1, 1, 5, 1, 2, 3, 1, 3, 3, 2, 3, 2, 2, 0, 1, 3, 0, 4, 2, 2, 4, 3,\n",
       "       2, 3, 0, 2, 2, 2, 3, 3, 3, 3, 4, 3, 0, 2, 4, 3, 2, 3, 3, 1, 1, 2,\n",
       "       1, 2, 3, 2, 3, 2, 4, 3, 3, 2, 4, 3, 2, 2, 2, 2, 2, 2, 3, 2, 0, 2,\n",
       "       3, 2, 3, 6, 3, 3, 3, 3, 2, 2, 0, 4, 3, 3, 5, 3, 1, 2, 4, 3, 3, 5,\n",
       "       2, 2, 1, 3, 3, 2, 0, 2, 3, 3, 2, 3, 1, 2, 3, 3, 4, 3, 3, 3, 3, 3,\n",
       "       1, 2, 2, 2, 2, 3, 3, 1, 1, 3, 3, 3, 2, 2, 2, 2, 3, 2, 1, 3, 2, 3,\n",
       "       3, 3, 1, 2, 4, 3, 3, 2, 1, 3, 3, 1, 2, 0, 2, 3, 3, 2, 2, 2, 3, 3,\n",
       "       2, 2, 2, 3, 2, 2, 2, 2, 1, 1, 3, 0, 1, 0, 3, 3, 1, 2, 2, 2, 3, 3,\n",
       "       3, 3, 3, 4, 3, 2, 2, 0, 1, 4, 2, 2, 3, 3, 4, 2, 3, 5, 2, 3, 3, 3,\n",
       "       3, 3, 0, 2, 2, 3, 4, 4, 3, 4, 4, 2, 3, 3, 2, 1, 3, 3, 3, 3, 3, 0,\n",
       "       1, 3, 2, 1, 3, 3, 2, 3, 4, 0, 1, 4, 1, 3, 0, 4, 3, 2, 2, 5, 1, 6,\n",
       "       3, 3, 2, 2, 2, 2, 3, 2, 3, 1, 3, 2, 3, 2, 3, 2, 0, 1, 6, 4, 3, 2,\n",
       "       2, 1, 2, 2, 2, 2, 3, 0, 1, 3, 3, 1, 2, 2, 2, 4, 0, 3, 3, 3, 3, 2,\n",
       "       3, 3, 3, 2, 1, 5, 2, 0, 4, 3, 4, 3, 3, 3, 2, 4, 2, 0, 3, 0, 0, 3,\n",
       "       3, 3, 2, 5, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 6, 1, 5, 4, 3, 3,\n",
       "       4, 3, 2, 2, 2, 3, 3, 4, 3, 2, 0, 5, 2, 1, 2, 3, 2, 2, 0, 2, 3, 3,\n",
       "       4, 3, 2, 2, 6, 0, 0, 3, 3, 2, 2, 0, 2, 1, 3, 3, 2, 0, 2, 2, 3, 2,\n",
       "       3, 4, 3, 4, 4, 3, 1, 3, 3, 6, 3, 3, 6, 4, 1, 2, 0, 0, 2, 1, 2, 0,\n",
       "       2, 3, 1, 5, 2, 5, 2, 0, 3, 5, 0, 4, 2, 2, 1, 0, 1, 1, 3, 2, 3, 3,\n",
       "       2, 3, 5, 2, 2, 3, 3, 2, 3, 6, 5, 2, 2, 3, 3, 2, 2, 3, 2, 3, 3, 3,\n",
       "       4, 4, 4, 2, 3, 4, 3, 3, 2, 5, 2, 1, 4, 6, 3, 4, 0, 2, 2, 3, 0, 3,\n",
       "       2, 0, 4, 3, 1, 3, 3, 3, 3, 3, 0, 1, 2, 2, 2, 3, 6, 2, 4, 2, 2, 2,\n",
       "       2, 3, 3, 2, 3, 2, 6, 2, 2, 0, 1, 3, 3, 0, 4, 3, 3, 1, 1, 2, 1, 4,\n",
       "       3, 2, 3, 3, 5, 2, 2, 3, 2, 0, 3, 3, 2, 4, 3, 2, 2, 2, 3, 6, 2, 3,\n",
       "       1, 1, 0, 3, 2, 3, 3, 3, 5, 3, 2, 4, 1, 4, 4, 1, 3, 1, 3, 1, 3, 2,\n",
       "       0, 2], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cc803b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the FRNN classifier, construct the model, and query on the test set.\n",
    "clf = FRNN(preprocessors=(RangeNormaliser(), ))\n",
    "model = clf(X_train, y_train)\n",
    "scores = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09ac0530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.7570373869371759\n",
      "7\n",
      "[[0.14287906 0.1430104  0.142788   ... 0.14278691 0.14278143 0.14268695]\n",
      " [0.14290388 0.14302379 0.14278631 ... 0.14277629 0.14278307 0.14266518]\n",
      " [0.14288094 0.143019   0.14277448 ... 0.1427848  0.14279966 0.14266866]\n",
      " ...\n",
      " [0.14290179 0.14299082 0.14283401 ... 0.14277546 0.14278601 0.14264363]\n",
      " [0.14287319 0.14307461 0.14276218 ... 0.14277611 0.14279623 0.14267299]\n",
      " [0.14290386 0.14301149 0.1428255  ... 0.14278934 0.14275216 0.1426576 ]]\n",
      "AUROC: 0.7570373869371759\n"
     ]
    }
   ],
   "source": [
    "# Convert scores to probabilities and calculate the AUROC.\n",
    "probabilities = probabilities_from_scores(scores)\n",
    "auroc = roc_auc_score(y_test, probabilities, multi_class='ovo')\n",
    "print('AUROC:', auroc)\n",
    "\n",
    "# Select classes with the highest scores and calculate the accuracy.\n",
    "probabilities = probabilities_from_scores(scores)\n",
    "auroc = roc_auc_score(y_test, probabilities, multi_class='ovo')\n",
    "print(len(probabilities[0]))\n",
    "print(probabilities)\n",
    "print('AUROC:', auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa14b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0856cd64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
