{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db854c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a52f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indiscernibility(matrix,y):\n",
    "    df=pd.DataFrame(matrix)\n",
    "    # this will return the list of columns     \n",
    "    y=list(df.columns)\n",
    "    grouped_df=df.groupby(y)\n",
    "    \n",
    "    ind_R=list(list())\n",
    "    for key, item in grouped_df:\n",
    "#         print(grouped_df.get_group(key), \"\\n\",grouped_df.get_group(key).index ,\"\\n\\n\")\n",
    "        lis=[]\n",
    "        for i in grouped_df.get_group(key).index:\n",
    "            lis.append(i)\n",
    "        ind_R.append(list(lis))\n",
    "    return ind_R\n",
    "\n",
    "def encoding_discourse_type(x):\n",
    "    if x==\"Lead\":\n",
    "        return 0\n",
    "    if x==\"Position\":\n",
    "        return 1\n",
    "    if x==\"Evidence\":\n",
    "        return 2\n",
    "    if x==\"Claim\":\n",
    "        return 3\n",
    "    if x==\"Concluding Statement\":\n",
    "        return 4\n",
    "    if x==\"Counterclaim\":\n",
    "        return 5\n",
    "    if x=='Rebuttal':\n",
    "        return 6\n",
    "    \n",
    "def stemming_stopwords_removing(df):\n",
    "    corpus=[]\n",
    "    for i in range(len(df)):\n",
    "        review=re.sub('[^a-zA-Z]',' ',df[\"discourse_text\"][i])\n",
    "        review=review.lower()\n",
    "        review=review.split()\n",
    "        ps=PorterStemmer()\n",
    "        all_stopwords=stopwords.words(\"english\")\n",
    "        review=[ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
    "        review=' '.join(review)\n",
    "        corpus.append(review)\n",
    "    return corpus\n",
    "\n",
    "# storing the total occurrence.......\n",
    "def get_total_index_words(corpus):\n",
    "    index_word={}\n",
    "    for i in corpus:\n",
    "        s=i.split()\n",
    "        for j in s:\n",
    "            if j not in index_word:\n",
    "                index_word[j]=1\n",
    "            else:\n",
    "                index_word[j]+=1\n",
    "    return index_word\n",
    "    \n",
    "def get_values(dataset,threshold=1):\n",
    "    \n",
    "    # taking sample of 20 documents for lead category....\n",
    "    df = dataset\n",
    "\n",
    "    total_corpus = stemming_stopwords_removing(df)\n",
    "    # print(total_corpus)\n",
    "\n",
    "    # getting total index words and their count in the taken sample as a dict\n",
    "    total_index_words = get_total_index_words(total_corpus)\n",
    "    # print(len(lead_index_words))\n",
    "\n",
    "    # Creating a list of total keywords before filtering..\n",
    "    total_keywords = list(total_index_words.keys())\n",
    "\n",
    "    # Creating a matrix of width equals len(lead_keywords)\n",
    "    matrix=np.zeros((len(df),len(total_keywords)))\n",
    "\n",
    "    \n",
    "    # Storing occurrence of each term in each document respectively\n",
    "    for i in range(len(total_corpus)):\n",
    "        s = total_corpus[i].split()\n",
    "        for h in s:\n",
    "            j = total_keywords.index(h)\n",
    "            matrix[i,j] += 1\n",
    "\n",
    "\n",
    "    # Storing their weights....\n",
    "    weighted_matrix = np.copy(matrix)\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        for j in range(len(total_keywords)):\n",
    "            weighted_matrix[i,j] = weighted_matrix[i,j] / total_index_words[total_keywords[j]]\n",
    "    #            print(weighted_matrix[i,j])\n",
    "\n",
    "\n",
    "\n",
    "    # FILTERING WEIGHTS with a threshold.......\n",
    "    valid_index = []\n",
    "    for i in range(len(df)):\n",
    "        for j in range(len(total_keywords)):\n",
    "            if weighted_matrix[i,j] >= threshold:\n",
    "                valid_index.append(j)\n",
    "\n",
    "    # removing duplicates and storing them in a list.......    \n",
    "    valid_index = list(set(valid_index))\n",
    "\n",
    "\n",
    "    # # Storing the final keywords.... \n",
    "    valid_index_words = []\n",
    "    for i in range(len(valid_index)):\n",
    "        valid_index_words.append(total_keywords[valid_index[i]])\n",
    "    # print(valid_lead_index_words)\n",
    "\n",
    "    return total_keywords,total_index_words,matrix, weighted_matrix, valid_index, valid_index_words\n",
    "\n",
    "def get_test_matrix(dataset,total_sample_keywords,threshold=1):\n",
    "      # taking sample of 20 documents for lead category....\n",
    "    df = dataset\n",
    "\n",
    "    total_corpus = stemming_stopwords_removing(df)\n",
    "    # print(total_corpus)\n",
    "\n",
    "    # getting total index words and their count in the taken sample as a dict\n",
    "    total_index_words = get_total_index_words(total_corpus)\n",
    "\n",
    "\n",
    "    # Creating a list of total keywords before filtering..\n",
    "    total_keywords = total_sample_keywords\n",
    "    # print(lead_keywords)\n",
    "\n",
    "    # Creating a matrix of width equals len(lead_keywords)\n",
    "    matrix=np.zeros((len(df),len(total_keywords)))\n",
    "    print(len(df),len(total_keywords))\n",
    "\n",
    "    \n",
    "    # Storing occurrence of each term in each document respectively\n",
    "    for i in range(len(total_corpus)):\n",
    "        s = total_corpus[i].split()\n",
    "        for h in s:\n",
    "            try:\n",
    "                j = total_keywords.index(h)\n",
    "                matrix[i,j] += 1\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "    # Storing their weights....\n",
    "    weighted_matrix = np.copy(matrix)\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        for j in range(len(total_keywords)):\n",
    "            try:\n",
    "                weighted_matrix[i,j] = weighted_matrix[i,j] / total_index_words[total_keywords[j]]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "\n",
    "    # FILTERING WEIGHTS with a threshold.......\n",
    "    valid_index = []\n",
    "    for i in range(len(df)):\n",
    "        for j in range(len(total_keywords)):\n",
    "            if weighted_matrix[i,j] >= threshold:\n",
    "                valid_index.append(j)\n",
    "\n",
    "    # removing duplicates and storing them in a list.......    \n",
    "    valid_index = list(set(valid_index))\n",
    "    \n",
    "\n",
    "\n",
    "    # # Storing the final keywords.... \n",
    "    valid_index_words = []\n",
    "    for i in range(len(valid_index)):\n",
    "        valid_index_words.append(total_keywords[valid_index[i]])\n",
    "    # print(valid_lead_index_words)\n",
    "\n",
    "    return total_keywords,total_index_words,matrix, weighted_matrix, valid_index, valid_index_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c00fa51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_element_indiscernibility(matrix,index):\n",
    "    df=pd.DataFrame(matrix)\n",
    "    # this will return the list of columns     \n",
    "    y=list(df.columns)\n",
    "    \n",
    "    grouped_df=df.groupby(y[index])\n",
    "    \n",
    "    ind_R=list(list())\n",
    "    for key, item in grouped_df:\n",
    "    #print(grouped_df.get_group(key), \"\\n\",grouped_df.get_group(key).index ,\"\\n\\n\")\n",
    "        lis=[]\n",
    "        for i in grouped_df.get_group(key).index:\n",
    "            lis.append(i)\n",
    "        ind_R.append(list(lis))\n",
    "    return ind_R\n",
    "\n",
    "def indiscernibility(matrix,y):\n",
    "    if len(y)==0:\n",
    "        return []\n",
    "    df=pd.DataFrame(matrix)\n",
    "    # this will return the list of columns     \n",
    "    grouped_df=df.groupby(y)\n",
    "    ind_R=list(list())\n",
    "    for key, item in grouped_df:\n",
    "    #print(grouped_df.get_group(key), \"\\n\",grouped_df.get_group(key).index ,\"\\n\\n\")\n",
    "        lis=[]\n",
    "        for i in grouped_df.get_group(key).index:\n",
    "            lis.append(i)\n",
    "        ind_R.append(list(lis))\n",
    "    return ind_R\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a488a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>230.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>313.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>Some certain areas in the United States ban ph...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>402.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>When people have phones, they know about certa...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 2</td>\n",
       "      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>759.0</td>\n",
       "      <td>886.0</td>\n",
       "      <td>Driving is one of the way how to get around. P...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>395</td>\n",
       "      <td>E527586F851C</td>\n",
       "      <td>1.622749e+12</td>\n",
       "      <td>1305.0</td>\n",
       "      <td>1334.0</td>\n",
       "      <td>We should not text and drive.</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>226 227 228 229 230 231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>396</td>\n",
       "      <td>E527586F851C</td>\n",
       "      <td>1.622749e+12</td>\n",
       "      <td>1335.0</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>It is overall a distraction and could lead you...</td>\n",
       "      <td>Concluding Statement</td>\n",
       "      <td>Concluding Statement 1</td>\n",
       "      <td>232 233 234 235 236 237 238 239 240 241 242 24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>397</td>\n",
       "      <td>5F1CF4B91975</td>\n",
       "      <td>1.622748e+12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>Drivers should not be able to use cell phones ...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>398</td>\n",
       "      <td>5F1CF4B91975</td>\n",
       "      <td>1.622748e+12</td>\n",
       "      <td>89.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>Every day, people die in car accidents. Studie...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>399</td>\n",
       "      <td>5F1CF4B91975</td>\n",
       "      <td>1.622748e+12</td>\n",
       "      <td>212.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>A message or a phone call can affect how much ...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index            id  discourse_id  discourse_start  discourse_end  \\\n",
       "0        0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "1        1  423A1CA112E2  1.622628e+12            230.0          312.0   \n",
       "2        2  423A1CA112E2  1.622628e+12            313.0          401.0   \n",
       "3        3  423A1CA112E2  1.622628e+12            402.0          758.0   \n",
       "4        4  423A1CA112E2  1.622628e+12            759.0          886.0   \n",
       "..     ...           ...           ...              ...            ...   \n",
       "295    395  E527586F851C  1.622749e+12           1305.0         1334.0   \n",
       "296    396  E527586F851C  1.622749e+12           1335.0         1614.0   \n",
       "297    397  5F1CF4B91975  1.622748e+12              0.0           88.0   \n",
       "298    398  5F1CF4B91975  1.622748e+12             89.0          211.0   \n",
       "299    399  5F1CF4B91975  1.622748e+12            212.0          294.0   \n",
       "\n",
       "                                        discourse_text        discourse_type  \\\n",
       "0    Modern humans today are always on their phone....                  Lead   \n",
       "1    They are some really bad consequences when stu...              Position   \n",
       "2    Some certain areas in the United States ban ph...              Evidence   \n",
       "3    When people have phones, they know about certa...              Evidence   \n",
       "4    Driving is one of the way how to get around. P...                 Claim   \n",
       "..                                                 ...                   ...   \n",
       "295                      We should not text and drive.              Position   \n",
       "296  It is overall a distraction and could lead you...  Concluding Statement   \n",
       "297  Drivers should not be able to use cell phones ...              Position   \n",
       "298  Every day, people die in car accidents. Studie...                 Claim   \n",
       "299  A message or a phone call can affect how much ...              Evidence   \n",
       "\n",
       "         discourse_type_num                                   predictionstring  \n",
       "0                    Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
       "1                Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n",
       "2                Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n",
       "3                Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n",
       "4                   Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...  \n",
       "..                      ...                                                ...  \n",
       "295              Position 1                            226 227 228 229 230 231  \n",
       "296  Concluding Statement 1  232 233 234 235 236 237 238 239 240 241 242 24...  \n",
       "297              Position 1              0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  \n",
       "298                 Claim 1  16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 3...  \n",
       "299              Evidence 1  40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 5...  \n",
       "\n",
       "[300 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.read_csv(\"train.csv\")\n",
    "train_head=train.head(100)\n",
    "train_mid=train[300:400]\n",
    "train_tail=train.tail(100)\n",
    "train=pd.concat([train_head,train_tail,train_mid])\n",
    "train.reset_index(inplace=True)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8859707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# position=train[train[\"discourse_type\"]==\"Position\"].copy()\n",
    "# position.reset_index(inplace=True)\n",
    "\n",
    "# evidence=train[train[\"discourse_type\"]==\"Evidence\"].copy()\n",
    "# evidence.reset_index(inplace=True)\n",
    "\n",
    "# claim=train[train[\"discourse_type\"]==\"Claim\"].copy()\n",
    "# claim.reset_index(inplace=True)\n",
    "\n",
    "# counter_claim=train[train[\"discourse_type\"]==\"Counterclaim\"].copy()\n",
    "# counter_claim.reset_index(inplace=True)\n",
    "\n",
    "# rebuttal=train[train[\"discourse_type\"]==\"Rebuttal\"].copy()\n",
    "# rebuttal.reset_index(inplace=True)\n",
    "\n",
    "# lead=train[train[\"discourse_type\"]==\"Lead\"].copy()\n",
    "# lead.reset_index(inplace=True)\n",
    "\n",
    "# concluding_statement=train[train[\"discourse_type\"]==\"Concluding Statement\"].copy()\n",
    "# concluding_statement.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a190481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t=position.head(50)+evidence.head(50)+claim.head(50)+counter_claim.head(50)+rebuttal.head(50)+lead.head(50)+concluding_statement.head(50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "972f5ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now i will send my training data to get its list of final keywords after eliminating useles words from our list\n",
    "# using weighted matrix .....\n",
    "\n",
    "train_total_keywords,train_total_index_words, train_matrix, train_weighted_matrix, train_valid_index, train_final_keywords=get_values(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c527a589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(train_weighted_matrix))\n",
    "c_attr=train_weighted_matrix.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1f9e195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "train[\"Y\"]=train[\"discourse_type\"].copy()\n",
    "train[\"Y\"]=train[\"Y\"].apply(lambda x: encoding_discourse_type(x))\n",
    "y=train[\"Y\"].values\n",
    "print(len(y))\n",
    "dec_attr=y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c501421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.25       0.11111111 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         1.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "shape of old training attribute (300, 1192)\n",
      "\n",
      "\n",
      "No of unique columns: 777\n",
      "[[1.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.25       0.         0.         ... 0.         0.         0.        ]\n",
      " [0.11111111 0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         1.         0.        ]]\n",
      "\n",
      " unique_col_table shape (777, 300)\n",
      "\n",
      "new train attributes after preserving only one repetitve element:\n",
      " [[1.         0.25       0.11111111 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         1.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      "shape of new train attributes: (300, 777)\n",
      "number of repeated columns are:  215\n"
     ]
    }
   ],
   "source": [
    "# NOW REMOVING IDENTICAL COLUMNS FROM OUR MATRIX INPLACE.......\n",
    "print((c_attr))\n",
    "print(\"shape of old training attribute\",c_attr.shape)\n",
    "grp=np.column_stack(c_attr)\n",
    "temp=pd.DataFrame(grp)\n",
    "# print(temp.shape)\n",
    "\n",
    "temp_grp=indiscernibility(temp,list(temp.columns))\n",
    "# print(temp_grp)\n",
    "\n",
    "unique_columns=[]\n",
    "for i in range(len(temp_grp)):\n",
    "    unique_columns.append(temp_grp[i][0])\n",
    "    \n",
    "unique_columns.sort()\n",
    "print()\n",
    "# print(\"Unique columns:\",unique_columns)\n",
    "print(\"\\nNo of unique columns:\",len(unique_columns))\n",
    "\n",
    "unique_col_table=list(list())\n",
    "for i in unique_columns:\n",
    "    unique_col_table.append(list(grp[i]))\n",
    "unique_col_table=np.array(unique_col_table)\n",
    "# print(\"\\nunique_col_table:\")\n",
    "print(unique_col_table)\n",
    "print(\"\\n unique_col_table shape\",unique_col_table.shape)\n",
    "\n",
    "new_train_attributes=np.column_stack(unique_col_table)\n",
    "\n",
    "print(\"\\nnew train attributes after preserving only one repetitve element:\\n\",new_train_attributes)\n",
    "print(\"\\nshape of new train attributes:\",new_train_attributes.shape)\n",
    "c_attr=new_train_attributes.copy()\n",
    "\n",
    "repetitive_columns=list(set(list(range(len(c_attr[0]))))-set(unique_columns))\n",
    "print(\"number of repeated columns are: \", len(repetitive_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ecd075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_LA(R_e,ind):\n",
    "    nR_e=list(set())\n",
    "    nind=list(set())\n",
    "    ans=[]\n",
    "    for i in R_e:\n",
    "        nR_e.append(set(i))\n",
    "    for i in ind:\n",
    "        nind.append(set(i))\n",
    "    \n",
    "    for i in range(len(nind)):\n",
    "        for j in range(len(nR_e)):\n",
    "            if nind[i]&nR_e[j]==nind[i]:\n",
    "                for val in nind[i]:\n",
    "                    ans.append(val)\n",
    "    ans=list(set(ans))\n",
    "    return ans\n",
    "\n",
    "def calculate_dependency(R_e,ind,universe=list(range(len(dec_attr)))):\n",
    "     # here universe is set of all objects or terms.\n",
    "    if len(ind)==0:\n",
    "        return 0\n",
    "    ans=calculate_LA(R_e,ind)\n",
    "    return len(ans)/len( universe)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12dbf1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.25      , 0.11111111, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        1.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        2.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        1.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 1.        ,\n",
       "        3.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        2.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_attr=c_attr.copy()\n",
    "new_arr=np.array([])\n",
    "for i in range(len(combined_attr)):\n",
    "    new_arr=np.append(new_arr,np.append(combined_attr[i],dec_attr[i]))\n",
    "# now reshape it...\n",
    "new_arr=new_arr.reshape(len(dec_attr),len(c_attr[0])+1)\n",
    "combined_attr=new_arr.copy()\n",
    "combined_attr\n",
    "# df1=pd.DataFrame(combined_attr)\n",
    "# print(list(df1.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14434a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_e=single_element_indiscernibility(combined_attr,len(combined_attr[0])-1)\n",
    "# R_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e3c2316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{72}\n",
      "{72, 43}\n",
      "{72, 43, 148}\n",
      "{72, 43, 148, 4}\n",
      "{148, 4, 72, 105, 43}\n",
      "{148, 4, 72, 105, 43, 15}\n",
      "{80, 148, 4, 72, 105, 43, 15}\n",
      "{4, 72, 105, 43, 524, 15, 80, 148}\n",
      "{4, 72, 105, 43, 524, 15, 80, 148, 29}\n",
      "{4, 519, 72, 105, 43, 524, 15, 80, 148, 29}\n",
      "{4, 519, 72, 105, 43, 524, 15, 80, 148, 119, 29}\n",
      "{4, 102, 519, 72, 105, 43, 524, 15, 80, 148, 119, 29}\n",
      "{4, 102, 519, 72, 105, 43, 524, 15, 80, 148, 119, 29, 126}\n",
      "{4, 102, 519, 72, 105, 43, 524, 15, 80, 50, 148, 119, 29, 126}\n",
      "{4, 102, 519, 72, 105, 43, 524, 15, 80, 50, 148, 52, 119, 29, 126}\n",
      "{4, 519, 72, 524, 15, 80, 148, 88, 29, 102, 105, 43, 50, 52, 119, 126}\n",
      "{4, 519, 72, 524, 15, 80, 207, 148, 88, 29, 102, 105, 43, 50, 52, 119, 126}\n",
      "{4, 519, 72, 524, 15, 80, 207, 148, 88, 29, 102, 105, 43, 50, 51, 52, 119, 126}\n",
      "{4, 519, 72, 524, 15, 80, 207, 148, 88, 29, 102, 105, 43, 50, 51, 52, 119, 59, 126}\n",
      "{4, 519, 72, 524, 15, 80, 207, 148, 21, 88, 29, 102, 105, 43, 50, 51, 52, 119, 59, 126}\n",
      "{4, 519, 72, 524, 15, 80, 207, 148, 21, 88, 29, 102, 105, 43, 50, 51, 52, 119, 59, 126, 447}\n",
      "{4, 519, 72, 524, 15, 80, 207, 148, 21, 88, 25, 29, 102, 105, 43, 50, 51, 52, 119, 59, 126, 447}\n",
      "{4, 519, 72, 524, 15, 80, 207, 148, 21, 88, 25, 29, 102, 105, 43, 50, 51, 52, 119, 59, 127, 126, 447}\n",
      "{4, 519, 72, 524, 15, 80, 207, 148, 21, 88, 25, 29, 32, 102, 105, 43, 50, 51, 52, 119, 59, 127, 126, 447}\n",
      "{4, 519, 72, 524, 15, 80, 207, 148, 21, 88, 25, 29, 32, 102, 105, 42, 43, 50, 51, 52, 119, 59, 127, 126, 447}\n",
      "{4, 519, 72, 266, 524, 15, 80, 207, 148, 21, 88, 25, 29, 32, 102, 105, 42, 43, 50, 51, 52, 119, 59, 127, 126, 447}\n",
      "{4, 519, 72, 266, 524, 15, 80, 207, 148, 21, 88, 25, 29, 222, 32, 102, 105, 42, 43, 50, 51, 52, 119, 59, 127, 126, 447}\n",
      "{4, 70, 519, 72, 266, 524, 15, 80, 207, 148, 21, 88, 25, 29, 222, 32, 102, 105, 42, 43, 50, 51, 52, 119, 59, 127, 126, 447}\n",
      "{4, 70, 519, 72, 8, 266, 524, 15, 80, 207, 148, 21, 88, 25, 29, 222, 32, 102, 105, 42, 43, 50, 51, 52, 119, 59, 127, 126, 447}\n",
      "{67, 4, 70, 519, 72, 8, 266, 524, 15, 80, 207, 148, 21, 88, 25, 29, 222, 32, 102, 105, 42, 43, 50, 51, 52, 119, 59, 127, 126, 447}\n",
      "{67, 4, 70, 519, 72, 8, 266, 524, 15, 80, 207, 146, 148, 21, 88, 25, 29, 222, 32, 102, 105, 42, 43, 50, 51, 52, 119, 59, 127, 126, 447}\n",
      "{4, 519, 8, 266, 524, 14, 15, 146, 148, 21, 25, 29, 32, 42, 43, 50, 51, 52, 59, 447, 67, 70, 72, 207, 80, 88, 222, 102, 105, 119, 126, 127}\n",
      "{4, 519, 8, 266, 524, 14, 15, 146, 148, 21, 25, 26, 29, 32, 42, 43, 50, 51, 52, 59, 447, 67, 70, 72, 207, 80, 88, 222, 102, 105, 119, 126, 127}\n",
      "{4, 519, 8, 266, 524, 14, 15, 146, 148, 21, 25, 26, 29, 32, 36, 42, 43, 50, 51, 52, 59, 447, 67, 70, 72, 207, 80, 88, 222, 102, 105, 119, 126, 127}\n",
      "{4, 519, 8, 266, 524, 14, 15, 146, 148, 21, 25, 26, 29, 32, 36, 42, 43, 49, 50, 51, 52, 59, 447, 67, 70, 72, 207, 80, 88, 222, 102, 105, 119, 126, 127}\n",
      "{4, 519, 8, 266, 524, 14, 15, 146, 148, 21, 25, 26, 29, 32, 36, 42, 43, 44, 49, 50, 51, 52, 59, 447, 67, 70, 72, 207, 80, 88, 222, 102, 105, 119, 126, 127}\n",
      "{4, 519, 8, 266, 524, 14, 15, 146, 148, 21, 25, 26, 29, 32, 36, 42, 43, 44, 49, 50, 51, 52, 59, 447, 67, 70, 72, 207, 80, 88, 222, 102, 105, 108, 119, 126, 127}\n",
      "{4, 519, 8, 266, 524, 14, 15, 146, 148, 21, 25, 26, 29, 32, 36, 42, 43, 44, 49, 50, 51, 52, 59, 447, 67, 70, 72, 78, 207, 80, 88, 222, 102, 105, 108, 119, 126, 127}\n",
      "{4, 519, 8, 266, 524, 14, 15, 146, 148, 21, 25, 26, 29, 32, 36, 42, 43, 44, 49, 50, 51, 52, 59, 447, 67, 70, 72, 78, 207, 80, 85, 88, 222, 102, 105, 108, 119, 126, 127}\n",
      "{4, 519, 8, 266, 524, 14, 15, 146, 148, 21, 25, 26, 29, 32, 36, 42, 43, 44, 49, 50, 51, 52, 59, 447, 67, 70, 72, 78, 207, 80, 85, 88, 222, 102, 105, 108, 119, 126, 127}\n",
      "1.0\n",
      "Old conditional matrix before reducing:\n",
      " [[1.         0.25       0.11111111 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         1.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      "New conditional matrix after reducing:\n",
      " [[0.00816327 0.         0.01388889 ... 0.         0.         0.        ]\n",
      " [0.00408163 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.00408163 0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.00408163 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.00408163 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.00408163 0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      "Decision matrix is:\n",
      " [0 1 2 2 3 2 2 3 2 4 1 3 2 4 0 1 3 2 5 6 4 0 1 3 3 3 3 2 3 2 3 2 4 0 1 3 2\n",
      " 3 2 3 2 4 0 3 3 3 1 3 2 3 2 3 2 4 1 3 3 3 2 2 3 2 4 0 1 3 2 3 2 2 4 1 3 2\n",
      " 3 2 3 2 4 0 1 3 2 3 2 0 1 3 2 3 2 4 1 0 3 2 5 2 4 0 2 2 4 0 1 3 3 2 3 2 3\n",
      " 4 1 3 3 3 2 3 3 2 2 3 4 0 1 3 3 3 3 2 3 2 3 2 4 1 3 3 3 2 3 2 3 2 5 6 4 0\n",
      " 1 3 3 3 3 2 3 2 3 2 4 0 1 3 3 3 2 2 2 4 0 1 3 3 3 2 2 2 4 0 1 3 3 3 2 3 2\n",
      " 3 3 2 5 6 4 0 3 3 2 2 2 1 2 4 6 3 2 3 2 3 2 0 1 2 2 3 2 3 4 0 1 3 2 3 2 5\n",
      " 6 4 1 2 3 2 3 2 3 2 4 0 1 3 3 3 2 3 2 3 2 4 1 3 2 5 3 2 2 3 0 1 3 2 3 2 5\n",
      " 2 5 2 4 0 1 3 2 2 3 4 0 3 3 3 2 3 3 2 5 2 6 2 4 1 0 3 3 3 2 3 2 3 2 2 3 1\n",
      " 4 1 3 2]\n"
     ]
    }
   ],
   "source": [
    "# Feature selection through QUICK REDUCT ALGO\n",
    "C=[]\n",
    "for i in range(len(c_attr[0])):\n",
    "    C.append(i)\n",
    "C=set(C)\n",
    "R=set()\n",
    "T=set()\n",
    "\n",
    "dep_R=0\n",
    "dep_T=0\n",
    "dep_Re=calculate_dependency(R_e,indiscernibility(c_attr,list(C)))\n",
    "reduct=True\n",
    "while dep_R!=1:\n",
    "    T=R.copy()\n",
    "    for x in C-R:\n",
    "        dep_R=calculate_dependency(R_e,indiscernibility(c_attr,list(R.union({x}))))\n",
    "        if dep_R>dep_T:\n",
    "            T=R.union({x}).copy()\n",
    "            dep_T=calculate_dependency(R_e,indiscernibility(c_attr,list(T)))\n",
    "    if len(R)==len(T):\n",
    "        reduct=False\n",
    "        break\n",
    "    R=T.copy()\n",
    "    \n",
    "    print(R)\n",
    "    dep_R=calculate_dependency(R_e,indiscernibility(c_attr,list(R)))\n",
    "    \n",
    "print(R)\n",
    "print(calculate_dependency(R_e,indiscernibility(c_attr,list(R))))\n",
    "\n",
    "# computing new reduced matrix\n",
    "print(\"Old conditional matrix before reducing:\\n\",c_attr)\n",
    "new_matrix=c_attr[:,list(R)]\n",
    "c_attr=new_matrix.copy()\n",
    "print(\"\\nNew conditional matrix after reducing:\\n\",new_matrix)\n",
    "# Note: decision attribute will remain the same\n",
    "print(\"\\nDecision matrix is:\\n\",dec_attr)\n",
    "# computing new combined matrix\n",
    "\n",
    "# combined_attr=c_attr.copy()\n",
    "# new_arr=np.array([])\n",
    "# for i in range(len(combined_attr)):\n",
    "#     new_arr=np.append(new_arr,np.append(combined_attr[i],dec_attr[i]))\n",
    "# # now reshape it...\n",
    "# new_arr=new_arr.reshape(len(dec_attr),len(c_attr[0])+1)\n",
    "# combined_attr=new_arr.copy()\n",
    "# print(\"\\nNew combined matrix:\\n\",combined_attr)\n",
    "\n",
    "# Storing final reduced columns list\n",
    "reduced_col=list(R)\n",
    "\n",
    "# storing new_terms\n",
    "# new_terms=[]\n",
    "# for i in reduced_col:\n",
    "#     new_terms.append(terms[i])\n",
    "# print(\"\\nNew columns as per order:\\n\",new_terms)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fce8f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00816327 0.         0.01388889 ... 0.         0.         0.        ]\n",
      " [0.00408163 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.00408163 0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.00408163 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.00408163 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.00408163 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(c_attr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c070e9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from frlearn.base import probabilities_from_scores, select_class\n",
    "from frlearn.classifiers import FRNN\n",
    "from frlearn.feature_preprocessors import RangeNormaliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34699d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(c_attr, y, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaffabf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 39)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0ddb508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef3a6f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the FRNN classifier, construct the model, and query on the test set.\n",
    "clf = FRNN(preprocessors=(RangeNormaliser(), ))\n",
    "model = clf(X_train, y_train)\n",
    "scores = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76f0adcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.6735270127911606\n",
      "accuracy: 0.4266666666666667\n"
     ]
    }
   ],
   "source": [
    "# Convert scores to probabilities and calculate the AUROC.\n",
    "probabilities = probabilities_from_scores(scores)\n",
    "auroc = roc_auc_score(y_test, probabilities, multi_class='ovo')\n",
    "print('AUROC:', auroc)\n",
    "\n",
    "# Select classes with the highest scores and calculate the accuracy.\n",
    "classes = select_class(scores)\n",
    "accuracy = accuracy_score(y_test, classes)\n",
    "print('accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfaa0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bd1cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
